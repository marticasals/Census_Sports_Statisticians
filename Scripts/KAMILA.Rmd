---
title: "KAMILA"
author: "Mart√≠ Oliver"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

We load the necessary packages and set a seed.

```{r}
library(readxl)
library(dplyr)
citation("dplyr")
library(factoextra)
citation("factoextra")
library(kamila)
citation("kamila")
library(kableExtra)
library(compareGroups)
set.seed(4)
```

We load the data to analyze.

```{r}
bd <- read_excel("/Volumes/PENMARTI/ARTICLES/ARTICLE CENS/DB_ANALYSIS.xlsx")
head(bd)
```

We rename the variables to make them easier to identify.

```{r}
noms <- c("Gender","Age group","Country categorized","Type","Highest level of education","Years worked in sports statistics in academia","Profile","Currently belong to a sports statistics research group?","Total citations in GS","Total citations in GS since 2018","Total citations in GS acording H-index","Total citations in GS acording H-index since 2018","Number of citations of the article with most citations","Number of sports studied")
colnames(bd) <- noms
attach(bd)
```

We create a data frame with only the numerical variables.

```{r}
conInd <- bd %>% select(where(is.numeric)) %>% names(.)
conVars <- bd[, conInd]
summary(conVars)
str(conVars)
length(conVars)
```

We standardize the data of the numerical variables.

```{r}
rangeStandardize <- function(x) {
  (x - min(x)) / diff(range(x))
}
conVars <- as.data.frame(lapply(conVars, rangeStandardize))
```

We create a data frame with only the categorical variables.

```{r}
conInd_index <- which(names(bd) %in% conInd)
catVars <- bd[, -conInd_index]
summary(catVars)
str(catVars)
```

We convert them to factors.

```{r}
catVars <- catVars %>% mutate_if(sapply(catVars, is.character), as.factor)
str(catVars)
catVars <- as.data.frame(catVars)
```

We execute the KAMILA algorithm.

```{r}
set.seed(4)
numberOfClusters <- 2:10
kmResPs <- kamila(conVar=conVars, 
                  catFactor=catVars, 
                  numClust=numberOfClusters,
                  calcNumClust="ps",
                  predStrThresh = 0.65,
                  numInit = 10,
                  maxIter= 100)
```

We calculate the prediction strength values for each cluster.

```{r}
kmResPs$nClust$psValues
kable(kmResPs$nClust$psValues) %>%
 kable_classic(full_width = F, html_font = "Calibri", font_size = 12)
```

We create the plot with the different prediction strength values, which will help us determine the optimal number of clusters.

```{r}
psPlot <- with(kmResPs$nClust, qplot(numberOfClusters, psValues) +
                 geom_errorbar(aes(x = numberOfClusters, ymin = psValues - stdErrPredStr,
                                   ymax = psValues + stdErrPredStr), width = 0.25))
psPlot <- psPlot + geom_hline(yintercept = 0.65, lty = 2)
psPlot <- psPlot + scale_x_continuous(breaks = numberOfClusters) + ylim(0, 1.1)
psPlot + ggtitle("Prediction strength values according to the number of clusters") +
            theme(plot.title = element_text(hjust = 0.5)) + 
            xlab("Number of clusters") + ylab("Preduction strength (ps)")
```

We calculate the optimal number of clusters.

```{r}
#Optimal Number of clusters
kmResPs$nClust$bestNClust
```

The number of observations and the Euclidean distance between the two clusters is as follows:

```{r}
table(kmResPs$finalMemb)
dist(kmResPs$finalCenters)
```

We add a new variable that indicates which cluster each observation belongs to.

```{r}
length(kmResPs$finalMemb)==nrow(bd) #Must be TRUE
df.vars.all.complete.w5.km2 <- cbind(bd, Cluster = factor(kmResPs$finalMemb))
dim(df.vars.all.complete.w5.km2) 
table(df.vars.all.complete.w5.km2$Cluster)
```

```{r}
object <- list(
  data = conVars,
  cluster = df.vars.all.complete.w5.km2$Cluster)
library(purrr)
tibble(i=1,j=2) %>% 
  pwalk(function(i,j) {
    fviz_cluster(object, axes=c(i, j), geom="point", 
                 main="Cluster plot after applying the KAMILA algorithm to the full data",
                 xlab = "PC1 (69.1%)", ylab = "PC2 (15,8%)",
                 ellipse.alpha = 0.2, labelsize = 9, 
                 pointsize=1.0, repel = TRUE,
                 show.clust.cent = F)  %>% 
      print() 
  })
```

Now we remove the three outlier observations and run the algorithm again.

```{r}
bd <- bd[-28,]
bd <- bd[-34,]
bd <- bd[-70,]
```

We create a data frame with only the numerical variables.

```{r}
conInd <- bd %>% select(where(is.numeric)) %>% names(.)
conVars <- bd[, conInd]
summary(conVars)
str(conVars)
length(conVars)
```

We standardize the data of the numerical variables.

```{r}
rangeStandardize <- function(x) {
  (x - min(x)) / diff(range(x))
}
conVars <- as.data.frame(lapply(conVars, rangeStandardize))
```

We create a data frame with only the categorical variables.

```{r}
conInd_index <- which(names(bd) %in% conInd)
catVars <- bd[, -conInd_index]
summary(catVars)
str(catVars)
```

We convert them to factors.

```{r}
catVars <- catVars %>% mutate_if(sapply(catVars, is.character), as.factor)
str(catVars)
catVars <- as.data.frame(catVars)
```

```{r}
set.seed(4)
numberOfClusters <- 2:10
library(kamila)
citation("kamila")
kmResPs <- kamila(conVar=conVars, 
                  catFactor=catVars, 
                  numClust=numberOfClusters,
                  calcNumClust="ps",
                  predStrThresh = 0.65,
                  numInit = 10,
                  maxIter= 100)
```

We calculate the prediction strength values for each cluster.

```{r}
library(kableExtra)
kmResPs$nClust$psValues
kable(kmResPs$nClust$psValues) %>%
 kable_classic(full_width = F, html_font = "Calibri", font_size = 12)

#kable(kmResPs$nClust$psValues) %>%
#  kable_classic(table.attr = "style='display:inline'")

#p <- kable(kmResPs$nClust$psValues) %>%
#   kable_classic(full_width = F, html_font = "Calibri", font_size = 12)
#t(p)
```

We create the plot with the different prediction strength values, which will help us determine the optimal number of clusters.

```{r}
psPlot <- with(kmResPs$nClust, qplot(numberOfClusters, psValues) +
                 geom_errorbar(aes(x = numberOfClusters, ymin = psValues - stdErrPredStr,
                                   ymax = psValues + stdErrPredStr), width = 0.25))
psPlot <- psPlot + geom_hline(yintercept = 0.65, lty = 2)
psPlot <- psPlot + scale_x_continuous(breaks = numberOfClusters) + ylim(0, 1.1)
psPlot + ggtitle("Prediction strength values according to the number of clusters") +
            theme(plot.title = element_text(hjust = 0.5)) + 
            xlab("Number of clusters") + ylab("Preduction strength (ps)")
```

We calculate the optimal number of clusters.

```{r}
#Optimal Number of clusters
kmResPs$nClust$bestNClust
```

The number of observations and the Euclidean distance between the two clusters is as follows:

```{r}
table(kmResPs$finalMemb)
dist(kmResPs$finalCenters)
```

We add a new variable that indicates which cluster each observation belongs to.

```{r}
length(kmResPs$finalMemb)==nrow(bd) #Must be TRUE
df.vars.all.complete.w5.km2 <- cbind(bd, Cluster = factor(kmResPs$finalMemb))
dim(df.vars.all.complete.w5.km2) 
table(df.vars.all.complete.w5.km2$Cluster)
```

After determining that the optimal number is two clusters, the results, graphically, are as follows:

```{r}
object <- list(
  data = conVars,
  cluster = df.vars.all.complete.w5.km2$Cluster)
library(purrr)
tibble(i=1,j=2) %>% 
  pwalk(function(i,j) {
    fviz_cluster(object, axes=c(i, j), geom="point", 
                 main="Cluster plot after applying the KAMILA algorithm and removing outliers",
                 xlab = "PC1 (69.1%)", ylab = "PC2 (15,8%)",
                 ellipse.alpha = 0.2, labelsize = 15, 
                 pointsize=1.0, repel = TRUE,
                 show.clust.cent = F)  %>% 
      print() 
  })
```

```{r}
#What variables are important to create the clusters
library(rpart)
Cluster = factor(kmResPs$finalMemb)
tree_W5 <- df.vars.all.complete.w5.km2 %>% 
            rpart(Cluster ~ ., data=., method="class") 
tree_W5$variable.importance
```

**Characterization of the clusters**

```{r}
Cluster <- factor(kmResPs$finalMemb)
```

```{r}
compareGroups(Cluster~., data = df.vars.all.complete.w5.km2, method = 2, chisq.test.perm = T)
compareGroups(Cluster~., data = df.vars.all.complete.w5.km2, method = 2, chisq.test.perm = F)
```

Significant variables

*Comparison between age groups*

```{r}
comp_age_group <- compareGroups(Cluster ~ `Age group`, data = df.vars.all.complete.w5.km2, chisq.test.perm = F)
createTable(comp_age_group, show.p.mul = T)
```

*Comparison between categorized country*

```{r}
comp_country <- compareGroups(Cluster ~ `Country categorized`, data = df.vars.all.complete.w5.km2, chisq.test.perm = T)
createTable(comp_country, show.p.mul = T, show.p.ratio = T)
```

*Comparison between types of university*

```{r}
comp_type <- compareGroups(Cluster ~ Type, data = df.vars.all.complete.w5.km2)
createTable(comp_type, show.p.mul=TRUE)
```

*Comparison between years worked*

```{r}
comp_years_worked <- compareGroups(Cluster ~ `Years worked in sports statistics in academia`, data = df.vars.all.complete.w5.km2, chisq.test.perm = F)
createTable(comp_years_worked, show.p.mul=TRUE)
```

*Comparison with total citations*

```{r}
comp_tot_citations <- compareGroups(Cluster ~ `Total citations in GS` , data = df.vars.all.complete.w5.km2)
createTable(comp_tot_citations, show.p.mul=TRUE)
```

*Comparison with total citations since 2018*

```{r}
comp_tot_citations <- compareGroups(Cluster ~ `Total citations in GS since 2018` , data = df.vars.all.complete.w5.km2)
createTable(comp_tot_citations, show.p.mul=TRUE)
```

*Comparison with total citations according to the H-index*

```{r}
comp_tot_citations_acording_Hindex <- compareGroups(Cluster ~ `Total citations in GS acording H-index` , data = df.vars.all.complete.w5.km2)
createTable(comp_tot_citations_acording_Hindex, show.p.mul=TRUE)
```

*Comparison with total citations according to the H-index since 2018*

```{r}
comp_tot_citations_acording_Hindex <- compareGroups(Cluster ~ `Total citations in GS acording H-index since 2018` , data = df.vars.all.complete.w5.km2)
createTable(comp_tot_citations_acording_Hindex, show.p.mul=TRUE)
```

*Comparison with the citations of the most cited article*

```{r}
comp_article_most_citations <- compareGroups(Cluster ~ `Number of citations of the article with most citations` , data = df.vars.all.complete.w5.km2)
createTable(comp_article_most_citations, show.p.mul=TRUE)
```

Insignificant variables

*Comparison between gender*

```{r}
comp_gender <- compareGroups(Cluster ~ Gender, data = df.vars.all.complete.w5.km2, chisq.test.perm = F)
createTable(comp_gender, show.p.mul = T)
```

*Comparison between education level*

```{r}
comp_highest_level_education <- compareGroups(Cluster ~ `Highest level of education`, data = df.vars.all.complete.w5.km2, chisq.test.perm = F)
createTable(comp_highest_level_education, show.p.mul=TRUE)
```

*Comparison between profiles*

```{r}
comp_profile <- compareGroups(Cluster ~ Profile, data = df.vars.all.complete.w5.km2, chisq.test.perm = F)
createTable(comp_profile, show.p.mul=TRUE)
```

*Comparison with the number of sports*

```{r}
comp_sports_studied <- compareGroups(Cluster ~ `Number of sports studied` , data = df.vars.all.complete.w5.km2)
createTable(comp_sports_studied, show.p.mul=TRUE)
```

Boxplot of each numerical variable, stratified by cluster.

```{r}
(var_num1 <- ggplot(bd, aes(y = `Total citations in GS`, fill = Cluster)) + 
    geom_boxplot())
(var_num2 <- ggplot(bd, aes(y = `Total citations in GS since 2018`, fill = Cluster)) + 
    geom_boxplot())
(var_num3 <- ggplot(bd, aes(y = `Total citations in GS acording H-index`, fill = Cluster)) + 
    geom_boxplot())
(var_num4 <- ggplot(bd, aes(y = `Total citations in GS acording H-index since 2018`, fill = Cluster)) + 
    geom_boxplot())
(var_num5 <- ggplot(bd, aes(y = `Number of citations of the article with most citations`, fill = Cluster)) + 
    geom_boxplot())
(var_num6 <- ggplot(bd, aes(y = `Number of sports studied`, fill = Cluster)) + 
    geom_boxplot())
```